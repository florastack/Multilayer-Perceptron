{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdraRuNBsrIm"
   },
   "source": [
    "Block for linking files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNYUP4zanOP9"
   },
   "source": [
    "#MLP CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfZZvPT8lkOD"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from multiprocessing import reduction\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class MLP():\n",
    "    \n",
    "    #parameter_init_type = \"RANDOM\" => all parameters initialzed randomly\n",
    "    #parameter_init_type = \"ACTIV_SPEC\" => activation specific:  all parameters of an edge initialized based on\n",
    "    def __init__(self, M, D, C, hidden_activation_func_list, output_activation_func, cost_function = u.cat_cross_entropy, parameter_init_type = \"RANDOM\"):\n",
    "        self.M = M     # M =number of hidden units in hidden layers (width)\n",
    "        self.C = C     # C outputs (number of classes)        \n",
    "        self.D = D     # D inputs (number of x inputs)         \n",
    "        #for reporting on model\n",
    "        self.parameter_init_type = parameter_init_type \n",
    "        self.num_hid_layers = len(hidden_activation_func_list)             \n",
    "        self.activation_functions = hidden_activation_func_list        \n",
    "        #list of represent all layers in mlp\n",
    "        self.layers = self.create_layers(hidden_activation_func_list, output_activation_func, cost_function, parameter_init_type)\n",
    "\n",
    "    #create layer list here for model\n",
    "    #called in class constructor\n",
    "    #returns a list of all layers\n",
    "    #hard coded so that all layers have to have the same number hidden units but this could be changed\n",
    "    def create_layers(self, hidden_activation_func_list, output_activation_func, cost_function, init_type):\n",
    "        layers_list = []    #list of all layers\n",
    "        init_params = []    #list of parameters for each edge layer\n",
    "        #dimensions for parameter matrices with bias additions (one's col added to X too)\n",
    "        Dplusbias = self.D +1       #V dim = (D+1, M)\n",
    "        Mplusbias = self.M +1       #W dim = (M+1, C)\n",
    "        #account for case with no hidden layers (log regression)\n",
    "        if hidden_activation_func_list == None or len(hidden_activation_func_list) == 0  or self.M==0:\n",
    "            spec_final_edge = l.Edge(Dplusbias, self.C)\n",
    "            layers_list.append(spec_final_edge)       #create first edge (from X to first HU) and add to list\n",
    "            init_params.append(spec_final_edge.get_params())\n",
    "        else:\n",
    "            #create hidden layers: length of passed activation funcs determines numbre of hidden layers\n",
    "            #first edge has special dimensions\n",
    "            edge = l.Edge(Dplusbias, Mplusbias)\n",
    "            layers_list.append(edge)\n",
    "            final_index = len(hidden_activation_func_list)-1\n",
    "            for i,activation_function in enumerate(hidden_activation_func_list):  \n",
    "                \n",
    "                hid_layer = l.HiddenLayer(activation_function) \n",
    "                layers_list.append(hid_layer)\n",
    "                if init_type == \"ACTIVATION_SPECIFIC\":\n",
    "                    params = edge.get_params()\n",
    "                    #params = params * 0.1\n",
    "                    size_last_layer = Mplusbias if i != 0 else Dplusbias\n",
    "                    params_custom = activation_function.param_init_by_activ_type(params, size_last_layer)\n",
    "                    edge.set_params(params_custom)\n",
    "                elif init_type == \"AROUND_ZERO\":\n",
    "                    params = edge.get_params()\n",
    "                    params_custom = params * 0.1\n",
    "                    edge.set_params(params_custom)\n",
    "                init_params.append(edge.get_params()) \n",
    "\n",
    "                #create new edge\n",
    "                edge = l.Edge(Mplusbias, Mplusbias) if i != final_index else l.Edge(Mplusbias, self.C)\n",
    "                layers_list.append(edge)\n",
    "\n",
    "            init_params.append(edge.get_params()) #append final edge that wasn't specially parameterized\n",
    "\n",
    "        layers_list.append(l.OutputLayer(output_activation_func, cost_function))         #create output layer\n",
    "        self.init_params = init_params \n",
    "\n",
    "        return layers_list\n",
    "    \n",
    "    def print_model_summary(self):\n",
    "        print(\"-----Model summary:------------------\")\n",
    "        print(f'Number of Instances Trained On:  N = {self.N}')\n",
    "        print(f'Number of Inputs Trained On:  D = {self.D}')\n",
    "        print(f'Number of Hidden Units:  M = {self.M}')\n",
    "        print(f'Number of Classes:  C = {self.C}')\n",
    "        print(f'Parameter Initialization Type:  {self.parameter_init_type}')\n",
    "        print(f'Gradient Descent Learning Rate: {self.learn_rate}')\n",
    "        print(f'Gradient Descent Iterations: {self.gd_iterations}')\n",
    "        print(f'Layer Dropout Keep Unit Percentages: {self.dropout_p}') \n",
    "        print(f'Number of Hidden Units Layers: {self.num_hid_layers}')\n",
    "        print(\"Activation Functions: \")\n",
    "        for af in self.activation_functions:\n",
    "            print(type(af).__name__)\n",
    "\n",
    "    #Compute forward pass\n",
    "    def forward_pass(self, X):\n",
    "        self.activations = []\n",
    "        if self.dropout_p != None: keep_probs = self.dropout_p.copy()    \n",
    "        last_index = len(self.layers)-1\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            input = X if i == 0 else z               \n",
    "            z = layer.get_output(input)\n",
    "            if self.dropout_p != None and isinstance(layer, l.HiddenLayer):\n",
    "              keep_prob_p  = keep_probs.pop(0)\n",
    "              if keep_prob_p != 0:                                    #remove from list for this iteration\n",
    "                drop_mask = (np.random.rand(*z.shape) < keep_prob_p) / keep_prob_p    #create dropout mask, invert to make predict scaling unnecc\n",
    "                z *= drop_mask # drop!\n",
    "            if isinstance(layer, l.HiddenLayer): self.activations.append(z)    \n",
    "        yh = z \n",
    "        return yh\n",
    "    \n",
    "    #perform the backward pass\n",
    "    #return list(parameter_gradients)  \n",
    "    #note all dimensions here include bias ie M = M+1 from model creation\n",
    "    def backward_pass(self, X, Y, Yh):\n",
    "        layers = self.layers.copy()             #edges and activation layers\n",
    "        activations = self.activations.copy()   #outputs of each hidden layer\n",
    "        activations.insert(0, X)                #add x as the beginning input,         \n",
    "        params = collections.deque()            #a list of parameters for gradient decent later\n",
    "\n",
    "        #last layer and edge special case:,     \n",
    "        final_layer = layers.pop(-1)                #just to pop\n",
    "        dy = Yh - Y                    #N x C       #pderiv(Loss) wrt y\n",
    "        z = activations.pop()          #N x M       #need last activations \n",
    "        final_edge = layers.pop(-1)                 #get last edge with W\n",
    "        params_from_above = final_edge.get_params() #get the weights for hidden layer calculations\n",
    "        dw = np.dot(z.T, dy)/self.N    #M x C\n",
    "\n",
    "        params.append(dw)                           #save for grad desc\n",
    "        err_from_above = dy           #N x C  #cost so far, backprop\n",
    "    \n",
    "        #reverse the layers (propograte from back): encounter hidden unit layer, then edge, then next hidden unit layer, etc\n",
    "        for layer in reversed(layers): \n",
    "            \n",
    "            if isinstance(layer, l.HiddenLayer):\n",
    "                dzq = layer.get_af_deriv(z) #N x M #dzq should have dim of z\n",
    "\n",
    "                z = activations.pop(-1)  #N x M\n",
    "                dz = np.dot(err_from_above, params_from_above.T) #N x M    #params_abv will be set in the last iteration   \n",
    "                err_from_above = dz #backprop error to next layer\n",
    "\n",
    "            else: \n",
    "                dv =  np.dot(z.T, dz * dzq)/self.N #z should be activations from last layer DxM\n",
    "                params_from_above = layer.get_params()  #layer will be edge, get V\n",
    "                params.appendleft(dv)\n",
    "\n",
    "        params = list(params)         #params was a deque for efficiency, change back to list\n",
    "        return params\n",
    "\n",
    "    #Hyperparameter: sdropout_p will be a list of dropout percentages for each layer\n",
    "    def fit(self, X, Y, learn_rate=0.1, gd_iterations=50, dropout_p=None):\n",
    "        self.N = X.shape[0]\n",
    "\n",
    "        #for printing statistics\n",
    "        self.learn_rate = learn_rate\n",
    "        self.gd_iterations = gd_iterations\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        #bias implementation: ADD COLS of 1 to x\n",
    "        bias = np.ones((self.N,1), dtype=float)\n",
    "        X = np.append(X, bias, axis=1)\n",
    "        \n",
    "        def gradient(X, Y, params):    \n",
    "            Yh = self.forward_pass(X)     \n",
    "            params = self.backward_pass(X, Y, Yh)\n",
    "            return params\n",
    "        \n",
    "        #create GradientDescent obj here and pass it our HP's, then run GD\n",
    "        optimizer = gd.GradientDescent(learning_rate=learn_rate, max_iters=gd_iterations)\n",
    "        learned_params = optimizer.run(gradient, X, Y, self.init_params) #pass grad , x, ,y, initial params         \n",
    "        \n",
    "        #run through layers and set params\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer,l.Edge):\n",
    "                layer.set_params(learned_params.pop())\n",
    "        return self\n",
    "\n",
    "    #returns the PROBABILITIES of classes \n",
    "    # (output of softmax rather than one hot encoding)\n",
    "    def predict_probs(self, X): \n",
    "        N = X.shape[0]\n",
    "\n",
    "        bias = np.ones((N,1), dtype=float)      #must add bias\n",
    "        X = np.append(X, bias, axis=1)\n",
    "        yh = self.forward_pass(X)               #compute through layers of functions\n",
    "\n",
    "        return yh     \n",
    "\n",
    "    def predict(self, X): \n",
    "        N = X.shape[0]\n",
    "\n",
    "        bias = np.ones((N,1), dtype=float)      #must add bias\n",
    "        X = np.append(X, bias, axis=1)\n",
    "        yh_probs = self.forward_pass(X)         #compute through layers of functions\n",
    "\n",
    "        def one_hot(row):\n",
    "            #need to use argmax because to break ties\n",
    "            prediction_index = np.argmax(row, axis = 0) #get the index of most prob class, axis 0 bc single row\n",
    "            row.fill(0) #in place set all values to 0\n",
    "            row[prediction_index] =1\n",
    "\n",
    "            return row\n",
    "        \n",
    "        yh =np.apply_along_axis(one_hot, 1, yh_probs)\n",
    "        \n",
    "        return yh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4lW4pGhnT1c"
   },
   "source": [
    "#TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4235840,
     "status": "ok",
     "timestamp": 1649104495250,
     "user": {
      "displayName": "Alicia Nguyen",
      "userId": "03695225755160591430"
     },
     "user_tz": 240
    },
    "id": "Pi18FFb2PFh-",
    "outputId": "8fc90b76-f060-49d2-a337-5e861ce43464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 784), y=(60000, 10)\n",
      "Test: X=(10000, 784), y=(10000, 10)\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "++++++++++++++++++TASK 3_1: Varying number of depth  layers ++++++++++++++++++\n",
      "-----Model summary:------------------\n",
      "Number of Instances Trained On:  N = 60000\n",
      "Number of Inputs Trained On:  D = 784\n",
      "Number of Hidden Units:  M = 128\n",
      "Number of Classes:  C = 10\n",
      "Parameter Initialization Type:  ACTIVATION_SPECIFIC\n",
      "Gradient Descent Learning Rate: 0.1\n",
      "Gradient Descent Iterations: 750\n",
      "Layer Dropout Keep Unit Percentages: None\n",
      "Number of Hidden Units Layers: 2\n",
      "Activation Functions: \n",
      "ReLU\n",
      "ReLU\n",
      "-------------------------------------\n",
      "test accuracy: 0.822\n",
      "-------------------------------------\n",
      "++++++++++++++++++TASK 3_2: Different activation functions ++++++++++++++++++\n",
      "-----Model summary:------------------\n",
      "Number of Instances Trained On:  N = 60000\n",
      "Number of Inputs Trained On:  D = 784\n",
      "Number of Hidden Units:  M = 128\n",
      "Number of Classes:  C = 10\n",
      "Parameter Initialization Type:  ACTIVATION_SPECIFIC\n",
      "Gradient Descent Learning Rate: 0.1\n",
      "Gradient Descent Iterations: 750\n",
      "Layer Dropout Keep Unit Percentages: None\n",
      "Number of Hidden Units Layers: 2\n",
      "Activation Functions: \n",
      "tanh\n",
      "tanh\n",
      "-------------------------------------\n",
      "test accuracy: 0.8233\n",
      "-------------------------------------\n",
      "-----Model summary:------------------\n",
      "Number of Instances Trained On:  N = 60000\n",
      "Number of Inputs Trained On:  D = 784\n",
      "Number of Hidden Units:  M = 128\n",
      "Number of Classes:  C = 10\n",
      "Parameter Initialization Type:  ACTIVATION_SPECIFIC\n",
      "Gradient Descent Learning Rate: 0.1\n",
      "Gradient Descent Iterations: 750\n",
      "Layer Dropout Keep Unit Percentages: None\n",
      "Number of Hidden Units Layers: 2\n",
      "Activation Functions: \n",
      "LeakyReLU\n",
      "LeakyReLU\n",
      "-------------------------------------\n",
      "test accuracy: 0.8242\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#TASK 3_1: Varying numbers of depth  layers\n",
    "def task3_1(Xtrain, Xtest, Ytrain, Ytest):\n",
    "    print(\"++++++++++++++++++TASK 3_1: Varying number of depth  layers ++++++++++++++++++\")\n",
    "    Ctask = Ytrain.shape[1] #10 classes in FASHION-MINST dataset, should be one hot encoded\n",
    "    Ntask,Dtask = Xtrain.shape \n",
    "\n",
    "    #1)no hidden layers\n",
    "    hidlayer_activfunc_list1 = []\n",
    "    output_activation1 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_1_1 = MLP(M=0, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list1, output_activation_func=output_activation1)\n",
    "    # fit model \n",
    "    model3_1_1.fit(Xtrain, Ytrain, learn_rate=0.1, gd_iterations=550, dropout_p=None)\n",
    "    Yh1 = model3_1_1.predict(Xtest)  \n",
    "    #print stats\n",
    "    model3_1_1.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh1)\n",
    "    \n",
    "    #2)1 hidden layer, 128 hidden units\n",
    "    hidlayer_activfunc_list2 = []\n",
    "    hidlayer_activfunc_list2.append(af.ReLU())\n",
    "    output_activation2 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_1_2 = MLP(M=128, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list2, output_activation_func=output_activation2, parameter_init_type=\"ACTIVATION_SPECIFIC\")\n",
    "    # fit model \n",
    "    model3_1_2.fit(Xtrain, Ytrain, learn_rate=0.2, gd_iterations=500, dropout_p=None)\n",
    "    Yh2 = model3_1_2.predict(Xtest)  \n",
    "    #print stats\n",
    "    model3_1_2.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh2)\n",
    "\n",
    "    #3)2 hidden layers, 128 hidden units\n",
    "    hidlayer_activfunc_list3 = []\n",
    "    hidlayer_activfunc_list3.append(af.ReLU())\n",
    "    hidlayer_activfunc_list3.append(af.ReLU())\n",
    "    output_activation3 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_1_3 = MLP(M=128, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list3, output_activation_func=output_activation3, parameter_init_type=\"ACTIVATION_SPECIFIC\")\n",
    "    # fit model \n",
    "    model3_1_3.fit(Xtrain, Ytrain, learn_rate=0.1, gd_iterations=750, dropout_p=None)\n",
    "    Yh3 = model3_1_3.predict(Xtest)  \n",
    "    #print stats\n",
    "    model3_1_3.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh3)\n",
    "\n",
    "#TASK 3_2: Different activations\n",
    "def task3_2(Xtrain, Xtest, Ytrain, Ytest):\n",
    "    print(\"++++++++++++++++++TASK 3_2: Different activation functions ++++++++++++++++++\")\n",
    "\n",
    "    Ctask = Ytrain.shape[1] #10 classes in FASHION-MINST dataset\n",
    "    Ntask,Dtask = Xtrain.shape \n",
    "\n",
    "    #1) 2 layer Tanh\n",
    "    hidlayer_activfunc_list1 = []\n",
    "    hidlayer_activfunc_list1.append(af.tanh())\n",
    "    hidlayer_activfunc_list1.append(af.tanh())\n",
    "    output_activation1 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_2_1 = MLP(M=128, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list1, output_activation_func=output_activation1, parameter_init_type=\"ACTIVATION_SPECIFIC\")\n",
    "    # fit model \n",
    "    model3_2_1.fit(Xtrain, Ytrain, learn_rate=0.1, gd_iterations=750, dropout_p=None)\n",
    "    Yh1 = model3_2_1.predict(Xtest)  \n",
    "    #print stats\n",
    "    model3_2_1.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh1)\n",
    "\n",
    "    #2)2 layer leaky relu\n",
    "    hidlayer_activfunc_list2 = []\n",
    "    hidlayer_activfunc_list2.append(af.LeakyReLU())\n",
    "    hidlayer_activfunc_list2.append(af.LeakyReLU())\n",
    "    output_activation2 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_2_2 = MLP(M=128, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list2, output_activation_func=output_activation2, parameter_init_type=\"ACTIVATION_SPECIFIC\")\n",
    "    # fit model \n",
    "    model3_2_2.fit(Xtrain, Ytrain, learn_rate=0.1, gd_iterations=750, dropout_p=None)\n",
    "    Yh2 = model3_2_2.predict(Xtest)  \n",
    "    #print stats\n",
    "    model3_2_2.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh2)\n",
    "\n",
    "#TASK 3_3: 2 Hidden Layers, Relu, with DROPOUT\n",
    "def task3_3(Xtrain, Xtest, Ytrain, Ytest, layer_dropout_percents = [0.8, 0.8]):\n",
    "    print(\"++++++++++++++++++TASK 3_3: DROPOUT: 2 Hidden Layers with Relu ++++++++++++++++++\")\n",
    "\n",
    "    Ctask = Ytrain.shape[1] #10 classes in FASHION-MINST dataset\n",
    "    Ntask,Dtask = Xtrain.shape \n",
    "\n",
    "    hidlayer_activfunc_list1 = []\n",
    "    hidlayer_activfunc_list1.append(af.ReLU())\n",
    "    hidlayer_activfunc_list1.append(af.ReLU())\n",
    "    output_activation1 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_2_1 = MLP(M=128, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list1, output_activation_func=output_activation1, parameter_init_type=\"ACTIVATION_SPECIFIC\")\n",
    "    # fit model \n",
    "    model3_2_1.fit(Xtrain, Ytrain, learn_rate=0.2, gd_iterations=180, dropout_p=layer_dropout_percents)\n",
    "    Yh1 = model3_2_1.predict(Xtest)  \n",
    "    #print stats\n",
    "    model3_2_1.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh1)\n",
    "\n",
    "\n",
    "#TASK 3_4: 2 Hidden Layers, Relu, with UNNORMALIZED IMAGES\n",
    "def task3_4(Xtrain, Xtest, Ytrain, Ytest):\n",
    "    print(\"++++++++++++++++++TASK 3_4: UNNORMALIZED DATA: 2 Hidden Layers with Relu ++++++++++++++++++\")\n",
    "    \n",
    "    Ctask = Ytrain.shape[1] #10 classes in FASHION-MINST dataset\n",
    "    Ntask,Dtask = Xtrain.shape \n",
    "\n",
    "    hidlayer_activfunc_list1 = []\n",
    "    hidlayer_activfunc_list1.append(af.ReLU())\n",
    "    hidlayer_activfunc_list1.append(af.ReLU())\n",
    "    output_activation1 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_2_1 = MLP(M=128, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list1, output_activation_func=output_activation1)\n",
    "    # fit model \n",
    "    model3_2_1.fit(Xtrain, Ytrain, learn_rate=0.3, gd_iterations=150, dropout_p=None)\n",
    "    Yh1 = model3_2_1.predict(Xtest)  \n",
    "    #print stats\n",
    "    print(\"UNNORMALIZED DATA\")\n",
    "    model3_2_1.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh1)\n",
    "\n",
    "#TASK 3_6: Optimize\n",
    "def task3_6(Xtrain, Xtest, Ytrain, Ytest, hid_units, epoques, learning_rate, dropout_list):\n",
    "    print(\"++++++++++++++++++TASK 3_6: BEST: 2 Hidden Layers with Relu ++++++++++++++++++\")\n",
    "    Ctask = Ytrain.shape[1] #10 classes in FASHION-MINST dataset\n",
    "    Ntask,Dtask = Xtrain.shape \n",
    "    hidlayer_activfunc_list1 = []\n",
    "    hidlayer_activfunc_list1.append(af.ReLU())\n",
    "    hidlayer_activfunc_list1.append(af.ReLU())\n",
    "    hidlayer_activfunc_list1.append(af.ReLU())\n",
    "    output_activation1 = af.SoftMax()\n",
    "    #create model object\n",
    "    model3_2_1 = MLP(M=hid_units, D=Dtask, C=Ctask, hidden_activation_func_list=hidlayer_activfunc_list1, output_activation_func=output_activation1, parameter_init_type = \"ACTIVATION_SPECIFIC\")\n",
    "    # fit model \n",
    "    model3_2_1.fit(Xtrain, Ytrain, learning_rate, gd_iterations=epoques, dropout_p=dropout_list)\n",
    "    Yh1 = model3_2_1.predict(Xtest)  \n",
    "    #print stats\n",
    "    model3_2_1.print_model_summary()\n",
    "    u.evaluate_acc(Ytest, Yh1)\n",
    "    print(\"Accuracy on TRAIN set (seen data):\")\n",
    "    Yh2 = model3_2_1.predict(Xtrain)  \n",
    "    #print stats\n",
    "    model3_2_1.print_model_summary()\n",
    "    u.evaluate_acc(Ytrain, Yh2)\n",
    "\n",
    "Xtrain, Ytrain, Xtest, Ytest = load_dataset() # load dataset\n",
    "Xtrain, Xtest = prep_pixels(Xtrain, Xtest)\n",
    "print('Train: X=%s, y=%s' % (Xtrain.shape, Ytrain.shape))\n",
    "print('Test: X=%s, y=%s' % (Xtest.shape, Ytest.shape))\n",
    "print(Xtrain.shape)\n",
    "print(Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlPLtwlmsYyR"
   },
   "source": [
    "# DATA LOADING\n",
    "(See Dataloading.py for more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjwMvQ1IDcjy"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "\t#reshape dataset to have a single channel\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 784))\n",
    "\ttestX = testX.reshape((testX.shape[0], 784))\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    " \n",
    "\treturn trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5dWhTGMDgZx"
   },
   "outputs": [],
   "source": [
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLPandTaskExecution.ipynb",
   "provenance": [
    {
     "file_id": "14ZKNy2Kx1KMVt70ztUgeBLgM_vB2ZqzR",
     "timestamp": 1649127237838
    },
    {
     "file_id": "1mdzaZyHwJYW4-Mp1XAn_BxguKahHJLAJ",
     "timestamp": 1649044640823
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
