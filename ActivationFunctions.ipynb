{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbPLX6R1S3da"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#HIDDEN LAYERS\n",
    "#parent class - defines functions all children have (to get the function and it's derivative)\n",
    "class ActivationFunction:\n",
    "    def get_func(self):\n",
    "        return self.function\n",
    "    def get_deriv(self):\n",
    "        return self.derivative\n",
    "    #to initialized parameter matrix based on activation type, matrix already initialized to random\n",
    "    def param_init_by_activ_type(self, V, M_last):\n",
    "        return self.param_initializer(V, M_last)\n",
    "\n",
    "#activation function (for hidden layer) \n",
    "class ReLU(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        reLU = lambda x: np.maximum(0, x)\n",
    "        self.function = reLU\n",
    "        #reLU_der = lambda x: 0 if x < 0 else 1 \n",
    "        reLU_der = lambda x: (x > 0) *1 # should be elementwise! make sure this is happening\n",
    "        self.derivative = reLU_der\n",
    "        #custom weight initialization based on af\n",
    "        self.param_initializer = lambda V, M_last : V * np.sqrt(2/M_last) #He init for ReLu\n",
    "\n",
    "#activation function (for hidden layer) \n",
    "class tanh(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        self.function = tanh\n",
    "        tanh_der = lambda x: 1 - np.tanh(x) * np.tanh(x) #TODO check elementwise here\n",
    "        self.derivative = tanh_der\n",
    "        #custom weight initialization based on af\n",
    "        self.param_initializer = lambda V, M_last : V * np.sqrt(1/M_last) #Xavier init for tanh\n",
    "\n",
    "#activation function (for hidden layer)\n",
    "#lambda is by default set to 0.2, but can be changed as HP\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    def __init__(self, lam=0.2):\n",
    "        self.function = self.leaky_ReLU\n",
    "        self.derivative = self.der_leaky_ReLU\n",
    "        self.lam = lam\n",
    "        #custom weight initialization based on af\n",
    "        self.param_initializer = lambda V, M_last : V * np.sqrt(2/M_last) #He init for ReLu\n",
    "        \n",
    "    #sum compacted, bc always > 0 with min addition\n",
    "    def leaky_ReLU(self, x):                           \n",
    "        y = np.where(x > 0, x, x * self.lam)                          \n",
    "        return y\n",
    "\n",
    "    def der_leaky_ReLU(self, x):\n",
    "        y = np.where((x>0), 1, self.lam)\n",
    "        return y\n",
    "\n",
    "#FINAL LAYER: \n",
    "#activation function (for final layer) multiclass classification\n",
    "class SoftMax(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "        self.function = softmax\n",
    "       \n",
    "class identity(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        identity = lambda x: x\n",
    "        self.function = identity\n",
    "\n",
    "#activation function (for final or hidden layer) binary classification\n",
    "class logistic(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        logistic = lambda x: 1./ (1. + np.exp(-x))\n",
    "        self.function = logistic\n",
    "        logistic_der = lambda x: np.multiply(x, (1 - x))\n",
    "        self.derivative = logistic_der"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ActivationFunctions.ipynb",
   "provenance": [
    {
     "file_id": "1OqYlW-xFt4vXlz16EnG3wy68EAxvOwSz",
     "timestamp": 1649045350671
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
